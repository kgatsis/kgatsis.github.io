<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="your description goes here" />
	<meta name="keywords" content="your,keywords,goes,here" />
	<meta name="author" content="Your Name" />
	<link rel="stylesheet" type="text/css" href="andreas02.css" media="screen" />
	<link rel="stylesheet" type="text/css" href="print.css" media="print" />
	<title>Konstantinos' website</title>
</head>

<body>

<div id="container">
	<div id="logo">
		<h1><a href="index.html">Konstantinos Gatsis</a></h1>
		<p> </p>
	</div>

	<div id="navitabs">
		<h2 class="hide">Sample navigation menu:</h2>
		<a class="navitab" href="index.html">Home</a><span class="hide"> | </span>
		<a class="navitab" href="research.html">Research</a><span class="hide"> | </span>
		<a class="navitab" href="publications.html">Publications</a><span class="hide"> | </span>
		<a class="activenavitab" href="teaching.html">Teaching</a><span class="hide"> | </span>
		<!-- <a class="navitab" href="coursework.html">Coursework</a><span class="hide"> | </span> -->
		<!-- <a class="navitab" href="personal.html">Personal</a><span class="hide"> | </span>-->
	</div>
	

	<div id="main">
	<br>
	<br>
	<h3><b>Courses at University of Oxford</b></h3>
	<br>
	<h3>&bull; Hillary Term 2021: C21 Dynamic Programming and Reinforcement Learning </h3>
	<p>
	<u>Syllabus</u>:
	The dynamic programming framework: states, actions, transitions, costs. Modeling dynamic decision making problems under uncertainty, such as shortest path problems. Definition of a policy and the value/cost-to-go function of a policy. Bellmanâ€™s principle of optimality and the dynamic programming algorithm. Infinite horizon problems, stationary policies, and the Bellman equation. Algorithms for solving infinite horizon problems: policy iteration and value iteration. Solving dynamic programming problems from data: introduction to reinforcement learning approaches. Learning value functions and learning policies.
	</p>
	<br>
	<h3>&bull; Hillary Term 2021: C20 Robust Control </h3>
	<p>
	<u>Syllabus</u>: 
Analysis of dynamical systems using Lyapunov functions, Stability analysis by Linear Matrix Inequalities (LMIs), Performance measures for systems with disturbances, Performance analysis by LMIs, Controller synthesis by semi-definite programming (SDP), Schur Complement, H2 Optimal Control, Linear Quadratic Regulator (LQR) and the Riccati equation, Stability of systems with uncertain dynamics by LMIs, Stability of systems with non-linearities using the S-procedure.
</p>
	<br>
	<h3>&bull; Hillary Term 2020: C20 Robust Control </h3>
	
<!-- Learning Outcomes:
1.	Understand basic notions of stability, performance, and robustness of dynamical systems
2.	Formulate guarantees of robustness through Lyapunov functions
3.	Pose robustness guarantees for linear systems using convex optimization (Linear Matrix Inequalities)
4.	Pose controller synthesis problems using convex optimization (Semi Definite Programs)
5.	Analyse complex dynamical systems with uncertainties and systems with non-linearities using convex optimization-->

	<br>
	<br>
	<h3><b>Courses at University of Pennsylvania </b></h3>
	<br>
	<h3>&bull; Spring 2019: ESE 680 Safe Learning for Control </h3>
	<p>
	With Manfred Morari and George J. Pappas
	<br>
	<br>
	This advanced topics course will expose students to new research problems in the application of data-driven learning methods for control systems and cyber-physical systems. In the first part of the course, basic theory and tools including but not limited to system identification, adaptive control, reinforcement learning, safe learning, and formal methods will be introduced. In the second part, the students will lead a presentation from a selected list of recent publications. Student evaluation will be based on the paper presentation as well as a project of the student's choice. Students may choose their topic of interest for the project, for example implement and evaluate the algorithmic tools discussed in this class, extend theoretical results of the presented papers, or apply the tools in their own research problem.
	<!-- <br>
	<br>
	<u>Prerequisites</u>: Linear systems (ESE 500) and Machine Learning (CIS 520), or permission from the instructors.  -->
	<br>
	<br>
	<u>List of topics</u>:
	<br>System identification, least squares, and persistence of excitation condition
	<br>Gaussian processes
	<br>Approximate dynamic programming/Reinforcement learning, Value function approximation, and Policy gradients
	<br>Learning for model-predictive control
	<br>Scenario-based control
	<br>Finite sample analysis and regret bounds  for control of linear systems
	<br>Safe learning for control
	<br>Safe learning with formal methods
	</p>
	<br>
		<br>
	<h3>&bull; Spring 2018: ESE 680 Learning for Control </h3>
	<p>
	With Manfred Morari and George J. Pappas
	</p>
	<br>
		<br>

	<h3>&bull; Fall 2017: ESE 680 Dynamic Programming and Optimal Control </h3>
	<p>
	<!--Please find a syllabus <a href="./ESE_680_Dynamic_Programming_and_Optimal_Control.pdf", target="_blank">here</a>. -->
	The course will describe the foundations of dynamic decision making under uncertainty. In this setup we are
dynamically observing the state of an environment (system) over time and are responsible for taking decisions at each time step in
an effort to steer the state of the system at future time steps. This evolution is subject to uncertainty, i.e., stochastic noise, as well as
incurs some cost, and we are interested in minimizing the costs accumulated over time. Hence, the optimal solution involves
accounting for expected future system behavior and planning how to react in a closed loop manner. Applications include traditional
control, robotics, dynamic resource allocation, etc.
<br>
	<br>
	<u>List of topics</u>:
	<br>Finite Horizon Markov Decision Process
	<br>Principle of Optimality and Dynamic Programming Algorithm
	<br>Linear Quadratic Problems. Riccati Equation. Certainty equivalence.
	<br>Limited lookahead and rollout approaches.
	<br>Discounted Infinite Horizon Markov Decision Process
	<br>Bellman equation
	<br>Value and Policy Iteration algorithms. Contraction properties
	<br>Approximate Dynamic Programming and Reinforcement Learning, Value function approximation, and Policy gradients
	<br>Simulation-based Algorithms. TD-learning, Q-learning and convergence properties. Exploration
	<br>Imperfect State Information and POMDPS 
	<br>Linear Quadratic Gaussian Control. Separation Principle
	</p>
	

	
	</div>


    
	<div id="footer">
		<p><strong>&copy; 2019 <a href="index.html">Konstantinos Gatsis</a></strong> | <!--<strong>Contact:</strong> kgatsis at seas.upenn.edu -->
		<!--| Template design by <a href="http://andreasviklund.com/">Andreas Viklund</a>--> </p>
	</div>

</div>
</body>
</html>