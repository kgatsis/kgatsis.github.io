<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Strict//EN" "http://www.w3.org/TR/xhtml1/DTD/xhtml1-strict.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en" lang="en">
<head>
	<meta http-equiv="content-type" content="text/html; charset=utf-8" />
	<meta name="description" content="your description goes here" />
	<meta name="keywords" content="your,keywords,goes,here" />
	<meta name="author" content="Your Name" />
	<link rel="stylesheet" type="text/css" href="andreas02.css" media="screen" />
	<link rel="stylesheet" type="text/css" href="print.css" media="print" />
	<title>Konstantinos' website</title>
</head>

<body>

<div id="container">
	<div id="logo">
		<h1><a href="index.html">Konstantinos Gatsis</a></h1>
		<p> </p>
	</div>

	<div id="navitabs">
		<h2 class="hide">Sample navigation menu:</h2>
		<a class="navitab" href="index.html">Home</a><span class="hide"> | </span>
		<a class="navitab" href="research.html">Research</a><span class="hide"> | </span>
		<a class="navitab" href="publications.html">Publications</a><span class="hide"> | </span>
		<a class="activenavitab" href="teaching.html">Teaching</a><span class="hide"> | </span>
		<!-- <a class="navitab" href="coursework.html">Coursework</a><span class="hide"> | </span> -->
		<!-- <a class="navitab" href="personal.html">Personal</a><span class="hide"> | </span>-->
	</div>
	

	<div id="main">
	<br>
	<br>
	<br>
	<h3><b>Spring 2019: ESE 680 Safe Learning for Control </b></h3>
	<p>
	In preparation with Manfred Morari and George J. Pappas
	<br>
	<br>
	This advanced topics course will expose students to new research problems in the application of data-driven learning methods for control systems and cyber-physical systems. In the first part of the course, basic theory and tools including but not limited to system identification, adaptive control, reinforcement learning, safe learning, and formal methods will be introduced. In the second part, the students will lead a presentation from a selected list of recent publications. Student evaluation will be based on the paper presentation as well as a project of the student's choice. Students may choose their topic of interest for the project, for example implement and evaluate the algorithmic tools discussed in this class, extend theoretical results of the presented papers, or apply the tools in their own research problem.
	<br>
	<br>
	<u>Prerequisites</u>: Linear systems (ESE 500) and Machine Learning (CIS 520), or permission from the instructors.  
	<br>
	<br>
	<u>Tentative list of topics</u>:
	<br>System identification, least squares, and persistence of excitation condition
	<br>Gaussian processes
	<br>Approximate dynamic programming/Reinforcement learning, Value function approximation, and Policy gradients
	<br>Learning for model-predictive control
	<br>Scenario-based control
	<br>Finite sample analysis and regret bounds  for control of linear systems
	<br>Safe learning for control
	<br>Safe learning with formal methods
	</p>
	<br>
		<br>
	<h3><b>Spring 2018: ESE 680 Learning for Control </b></h3>
	<p>
	With George J. Pappas and Manfred Morari. 
	</p>
	<br>
		<br>

	<h3><b>Fall 2017: ESE 680 Dynamic Programming and Optimal Control </b></h3>
	<p>
	<!--Please find a syllabus <a href="./ESE_680_Dynamic_Programming_and_Optimal_Control.pdf", target="_blank">here</a>. -->
	The course will describe the foundations of dynamic decision making under uncertainty. In this setup we are
dynamically observing the state of an environment (system) over time and are responsible for taking decisions at each time step in
an effort to steer the state of the system at future time steps. This evolution is subject to uncertainty, i.e., stochastic noise, as well as
incurs some cost, and we are interested in minimizing the costs accumulated over time. Hence, the optimal solution involves
accounting for expected future system behavior and planning how to react in a closed loop manner. Applications include traditional
control, robotics, dynamic resource allocation, etc.
<br>
	<br>
	<u>List of topics</u>:
	<br>Finite Horizon Markov Decision Process
	<br>Principle of Optimality and Dynamic Programming Algorithm
	<br>Linear Quadratic Problems. Riccati Equation. Certainty equivalence.
	<br>Limited lookahead and rollout approaches.
	<br>Discounted Infinite Horizon Markov Decision Process
	<br>Bellman equation
	<br>Value and Policy Iteration algorithms. Contraction properties
	<br>Approximate Dynamic Programming and Reinforcement Learning, Value function approximation, and Policy gradients
	<br>Simulation-based Algorithms. TD-learning, Q-learning and convergence properties. Exploration
	<br>Imperfect State Information and POMDPS 
	<br>Linear Quadratic Gaussian Control. Separation Principle
	</p>
	

	
	</div>


    
	<div id="footer">
		<p><strong>&copy; 2019 <a href="index.html">Konstantinos Gatsis</a></strong> | <strong>Contact:</strong> kgatsis at seas.upenn.edu 
		<!--| Template design by <a href="http://andreasviklund.com/">Andreas Viklund</a>--> </p>
	</div>

</div>
</body>
</html>